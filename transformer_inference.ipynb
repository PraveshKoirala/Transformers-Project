{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPugsacFAr2F2p4Ww6UMrZq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PraveshKoirala/Transformers-Project/blob/main/transformer_inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hzHUcsV682OL",
        "outputId": "8acef41e-fe13-4934-d94d-0463414d48f9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (1.12.1+cu113)\n",
            "Collecting torch\n",
            "  Downloading torch-1.13.0-cp38-cp38-manylinux1_x86_64.whl (890.2 MB)\n",
            "\u001b[K     |██████████████████████████████  | 834.1 MB 103.2 MB/s eta 0:00:01tcmalloc: large alloc 1147494400 bytes == 0x65ab2000 @  0x7f8014070615 0x5d631c 0x51e4f1 0x51e67b 0x4f7585 0x49ca7c 0x4fdff5 0x49caa1 0x4fdff5 0x49ced5 0x4f60a9 0x55f926 0x4f60a9 0x55f926 0x4f60a9 0x55f926 0x5d7c18 0x5d9412 0x586636 0x5d813c 0x55f3fd 0x55e571 0x5d7cf1 0x49ced5 0x55e571 0x5d7cf1 0x49ec69 0x5d7c18 0x49ca7c 0x4fdff5 0x49ced5\n",
            "\u001b[K     |████████████████████████████████| 890.2 MB 5.6 kB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch) (4.1.1)\n",
            "Collecting nvidia-cuda-runtime-cu11==11.7.99\n",
            "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "\u001b[K     |████████████████████████████████| 849 kB 52.9 MB/s \n",
            "\u001b[?25hCollecting nvidia-cublas-cu11==11.10.3.66\n",
            "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 317.1 MB 24 kB/s \n",
            "\u001b[?25hCollecting nvidia-cudnn-cu11==8.5.0.96\n",
            "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 557.1 MB 10 kB/s \n",
            "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu11==11.7.99\n",
            "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 21.0 MB 72.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel in /usr/local/lib/python3.8/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (0.38.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (57.4.0)\n",
            "Installing collected packages: nvidia-cublas-cu11, nvidia-cudnn-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, torch\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.12.1+cu113\n",
            "    Uninstalling torch-1.12.1+cu113:\n",
            "      Successfully uninstalled torch-1.12.1+cu113\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.13.1+cu113 requires torch==1.12.1, but you have torch 1.13.0 which is incompatible.\n",
            "torchtext 0.13.1 requires torch==1.12.1, but you have torch 1.13.0 which is incompatible.\n",
            "torchaudio 0.12.1+cu113 requires torch==1.12.1, but you have torch 1.13.0 which is incompatible.\u001b[0m\n",
            "Successfully installed nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 torch-1.13.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Configuration parameters"
      ],
      "metadata": {
        "id": "Apk5smKo672t"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3_DvPsfm61n5",
        "outputId": "34c00798-14b9-4220-d5d4-16b83c8efa7d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing config.py\n"
          ]
        }
      ],
      "source": [
        "%%file config.py\n",
        "# Max number of tokens\n",
        "dmodel = 512\n",
        "dim_val = dmodel  # This can be any value divisible by n_heads. 512 is used in the original transformer paper.\n",
        "target_seq_len = 1  # Length of the target sequence, i.e. how many time steps should your forecast cover\n",
        "n_encoder_layers = 4  # Number of times the encoder layer is stacked in the encoder\n",
        "n_decoder_layers = 4  # Number of times the decoder layer is stacked in the decoder\n",
        "n_heads = 8  # The number of attention heads (aka parallel attention layers). dim_val must be divisible by this number\n",
        "batch_size = 512\n",
        "\n",
        "enc_seq_len = 5  # length of input given to encoder. Can have any integer value.\n",
        "dec_seq_len = enc_seq_len-1  # length of input given to decoder. Can have any integer value.\n",
        "max_seq_len = enc_seq_len  # What's the longest sequence the model will encounter? Used to make the positional encoder\n",
        "epochs = 20\n",
        "lr = 0.001\n",
        "weight_decay = 0.0001\n",
        "ratio = 0.8\n",
        "DEVICE = \"cuda\"\n",
        "TIME_EMBEDDING = 32\n",
        "SEGMENT_EMBEDDING = 128\n",
        "DAY_EMBEDDING = 7\n",
        "NUM_DAY = 7\n",
        "NUM_SEGMENTS = 50\n",
        "NUM_TIME=96\n",
        "NUM_CONTINUOUS=8\n",
        "dim_continuous = dmodel - TIME_EMBEDDING - SEGMENT_EMBEDDING - DAY_EMBEDDING"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Datasets and Dataloader stuff"
      ],
      "metadata": {
        "id": "dmkE-SMl7E5E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "drive_prefix=\"/content/drive/MyDrive/Transformers/\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E6CprGTsFy5X",
        "outputId": "d09ccdd2-e76a-4e7e-f37b-9e8921958959"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install --upgrade pandas"
      ],
      "metadata": {
        "id": "-XO6ViaOF-84"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%file dataset.py\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import pandas as pd\n",
        "import config\n",
        "import numpy as np\n",
        "\n",
        "class MyDataset(Dataset):\n",
        "    def __init__(self, data, buckets):\n",
        "        self.buckets=buckets\n",
        "        self.data = data\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        cols = self.data.iloc[self.buckets[item]]\n",
        "        return  torch.tensor(cols[['time_window', 'dayofweek', 'segment_id_int',\n",
        "                                   'is_holiday', 'is_school_break', \n",
        "                                   'travel_time', 'darksky_temperature',\n",
        "                                   'darksky_humidity', \n",
        "                                   'darksky_precipitation_probability',\n",
        "                                   'traffic_speed', 'distance_m']].astype(np.float).values), \\\n",
        "                torch.tensor(cols[['delay_time']].iloc[:config.max_seq_len-1].astype(np.float).values), \\\n",
        "                torch.tensor(cols[['delay_time']].iloc[config.max_seq_len-1].astype(np.float).values)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buckets)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZV7YKS5S7Eah",
        "outputId": "9eb7ab8f-e7aa-4298-a70d-307938f808f4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing dataset.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%file metrics.py\n",
        "import numpy as np\n",
        "\n",
        "def get_mape(x, y):\n",
        "    return np.mean(np.abs((x-y)/x)) * 100\n",
        "\n",
        "\n",
        "def get_rmse(x, y):\n",
        "    return np.sqrt(np.mean(np.square(x - y)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vd4Sda_A6_Ek",
        "outputId": "60f2ab53-174d-4ace-d81a-fcd0f5b4cc1d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing metrics.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%file penc.py\n",
        "import torch\n",
        "import math\n",
        "from torch import nn, Tensor\n",
        "\n",
        "\n",
        "class PositionalEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Adapted from:\n",
        "    https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
        "    https://github.com/LiamMaclean216/Pytorch-Transfomer/blob/master/utils.py\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dropout: float = 0.1, max_seq_len: int = 5000, d_model: int = 512):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            dropout: the dropout rate\n",
        "            max_seq_len: the maximum length of the input sequences\n",
        "            d_model: The dimension of the output of sub-layers in the model\n",
        "                     (Vaswani et al, 2017)\n",
        "        \"\"\"\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        # Create constant positional encoding matrix with values\n",
        "        # dependent on position and i\n",
        "        position = torch.arange(max_seq_len).unsqueeze(1)\n",
        "\n",
        "        exp_input = torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model)\n",
        "\n",
        "        div_term = torch.exp(exp_input)  # Returns a new tensor with the exponential of the elements of exp_input\n",
        "\n",
        "        pe = torch.zeros(max_seq_len, d_model)\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)  # torch.Size([target_seq_len, dim_val])\n",
        "\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)  # torch.Size([target_seq_len, input_size, dim_val])\n",
        "\n",
        "        # register that pe is not a model parameter\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Tensor, shape [batch_size, enc_seq_len, dim_val]\n",
        "        \"\"\"\n",
        "\n",
        "        add = self.pe[:x.size(1), :].squeeze(1)\n",
        "\n",
        "        x = x + add\n",
        "\n",
        "        return self.dropout(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dfei135s7YgK",
        "outputId": "c9adc7a3-2d3e-4fd0-dd5a-c37dfbee6e37"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing penc.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%file time_series_transformer.py\n",
        "import torch\n",
        "from torch import nn, Tensor\n",
        "\n",
        "from penc import PositionalEncoder\n",
        "from config import *\n",
        "\n",
        "class TimeSeriesTransformer(nn.Module):\n",
        "    \"\"\"\n",
        "    This class implements a transformer model that can be used for times series\n",
        "    forecasting. This time series transformer model is based on the paper by\n",
        "    Wu et al (2020) [1]. The paper will be referred to as \"the paper\".\n",
        "    A detailed description of the code can be found in my article here:\n",
        "    https://towardsdatascience.com/how-to-make-a-pytorch-transformer-for-time-series-forecasting-69e073d4061e\n",
        "    In cases where the paper does not specify what value was used for a specific\n",
        "    configuration/hyperparameter, this class uses the values from Vaswani et al\n",
        "    (2017) [2] or from PyTorch source code.\n",
        "    Unlike the paper, this class assumes that input layers, positional encoding\n",
        "    layers and linear mapping layers are separate from the encoder and decoder,\n",
        "    i.e. the encoder and decoder only do what is depicted as their sub-layers\n",
        "    in the paper. For practical purposes, this assumption does not make a\n",
        "    difference - it merely means that the linear and positional encoding layers\n",
        "    are implemented inside the present class and not inside the\n",
        "    Encoder() and Decoder() classes.\n",
        "    [1] Wu, N., Green, B., Ben, X., O'banion, S. (2020).\n",
        "    'Deep Transformer Models for Time Series Forecasting:\n",
        "    The Influenza Prevalence Case'.\n",
        "    arXiv:2001.08317 [cs, stat] [Preprint].\n",
        "    Available at: http://arxiv.org/abs/2001.08317 (Accessed: 9 March 2022).\n",
        "    [2] Vaswani, A. et al. (2017)\n",
        "    'Attention Is All You Need'.\n",
        "    arXiv:1706.03762 [cs] [Preprint].\n",
        "    Available at: http://arxiv.org/abs/1706.03762 (Accessed: 9 March 2022).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 # input_size: int,\n",
        "                 dec_seq_len: int,\n",
        "                 max_seq_len: int,\n",
        "                 out_seq_len: int,\n",
        "                 dim_val: int,\n",
        "                 n_encoder_layers: int = 4,\n",
        "                 n_decoder_layers: int = 4,\n",
        "                 n_heads: int = 4,\n",
        "                 dropout_encoder: float = 0.2,\n",
        "                 dropout_decoder: float = 0.2,\n",
        "                 dropout_pos_enc: float = 0.2,\n",
        "                 dim_feedforward_encoder: int = 2048,\n",
        "                 dim_feedforward_decoder: int = 2048,\n",
        "                 ):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            input_size: int, number of input variables. 1 if univariate.\n",
        "            dec_seq_len: int, the length of the input sequence fed to the decoder\n",
        "            max_seq_len: int, length of the longest sequence the model will\n",
        "                         receive. Used in positional encoding.\n",
        "            out_seq_len: int, the length of the model's output (i.e. the target\n",
        "                         sequence length)\n",
        "            dim_val: int, aka d_model. All sub-layers in the model produce\n",
        "                     outputs of dimension dim_val\n",
        "            n_encoder_layers: int, number of stacked encoder layers in the encoder\n",
        "            n_decoder_layers: int, number of stacked encoder layers in the decoder\n",
        "            n_heads: int, the number of attention heads (aka parallel attention layers)\n",
        "            dropout_encoder: float, the dropout rate of the encoder\n",
        "            dropout_decoder: float, the dropout rate of the decoder\n",
        "            dropout_pos_enc: float, the dropout rate of the positional encoder\n",
        "            dim_feedforward_encoder: int, number of neurons in the linear layer\n",
        "                                     of the encoder\n",
        "            dim_feedforward_decoder: int, number of neurons in the linear layer\n",
        "                                     of the decoder\n",
        "        \"\"\"\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.dec_seq_len = dec_seq_len\n",
        "\n",
        "        # The time ranges from 0 to 23 indicating the hour of the day\n",
        "        self.time_embedding = nn.Embedding(num_embeddings=NUM_TIME, embedding_dim=TIME_EMBEDDING)\n",
        "        # the road segments ranges from 0 to ... specifying the road segments\n",
        "        self.segment_embedding = nn.Embedding(num_embeddings=NUM_SEGMENTS, embedding_dim=SEGMENT_EMBEDDING)\n",
        "        self.day_embedding = nn.Embedding(num_embeddings=NUM_DAY, embedding_dim=DAY_EMBEDDING)\n",
        "\n",
        "        # 1. create 'linear input layer' for 'encoder'\n",
        "        self.encoder_input_layer = nn.Linear(in_features=NUM_CONTINUOUS, out_features=dim_continuous)\n",
        "\n",
        "        # 2. create positional encoder\n",
        "        self.positional_encoding_layer = PositionalEncoder(d_model=dim_val,\n",
        "                                                           dropout=dropout_pos_enc,\n",
        "                                                           max_seq_len=max_seq_len)\n",
        "\n",
        "        # 3. create encoder layers using nn.TransformerDecoder\n",
        "        # The encoder layer used in the paper is identical to the one used by\n",
        "        # Vaswani et al (2017) on which the PyTorch module is based.\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=dim_val,\n",
        "                                                   nhead=n_heads,\n",
        "                                                   dim_feedforward=dim_feedforward_encoder,\n",
        "                                                   dropout=dropout_encoder,\n",
        "                                                   batch_first=True\n",
        "                                                   )\n",
        "\n",
        "        # It seems the option of passing a normalization instance is redundant\n",
        "        # in my case, because nn.TransformerEncoderLayer per default normalizes\n",
        "        # after each sub-layer (https://github.com/pytorch/pytorch/issues/24930).\n",
        "        self.encoder = nn.TransformerEncoder(encoder_layer=encoder_layer, num_layers=n_encoder_layers, norm=None)\n",
        "\n",
        "\n",
        "        # 4. create 'linear input layer' for decoder\n",
        "        self.decoder_input_layer = nn.Linear(in_features=1, out_features=dim_val)\n",
        "\n",
        "        # 5. create decoder layers using nn.TransformerDecoder\n",
        "        decoder_layer = nn.TransformerDecoderLayer(d_model=dim_val,\n",
        "                                                   nhead=n_heads,\n",
        "                                                   dim_feedforward=dim_feedforward_decoder,\n",
        "                                                   dropout=dropout_decoder,\n",
        "                                                   batch_first=True\n",
        "                                                   )\n",
        "        self.decoder = nn.TransformerDecoder(decoder_layer=decoder_layer, num_layers=n_decoder_layers, norm=None)\n",
        "\n",
        "        # 6. create 'linear mapping layer'\n",
        "        self.linear_mapping = nn.Linear(in_features=dim_val, out_features=out_seq_len)\n",
        "\n",
        "    def forward(self, src: Tensor, tgt: Tensor, src_mask: Tensor = None, tgt_mask: Tensor = None) -> Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            src: the encoder's output sequence. Shape: (S,E) for unbatched input,\n",
        "                 (S, N, E) if batch_first=False or (N, S, E) if\n",
        "                 batch_first=True, where S is the source sequence length,\n",
        "                 N is the batch size, and E is the feature number\n",
        "            tgt: the sequence to the decoder. Shape: (T,E) for unbatched input,\n",
        "                 (T, N, E)(T,N,E) if batch_first=False or (N, T, E) if\n",
        "                 batch_first=True, where T is the target sequence length,\n",
        "                 N is the batch size, E is the feature number.\n",
        "            src_mask: the mask for the src sequence to prevent the model from\n",
        "                      using data points from the target sequence\n",
        "            tgt_mask: the mask for the tgt sequence to prevent the model from\n",
        "                      using data points from the target sequence\n",
        "        \"\"\"\n",
        "        time_embedding = self.time_embedding(src[:,:,0].int())\n",
        "        day_embedding = self.day_embedding(src[:, :, 1].int())\n",
        "        segment_embedding = self.segment_embedding(src[:, :, 2].int())\n",
        "\n",
        "        src = self.encoder_input_layer(src[:,:,3:].float())\n",
        "        # concatenate them\n",
        "        src = torch.cat([time_embedding, day_embedding, segment_embedding, src], dim=2)\n",
        "        # add positional\n",
        "        src = self.positional_encoding_layer(src)\n",
        "\n",
        "        # Pass through all the stacked encoder layers in the encoder\n",
        "        # Masking is only needed in the encoder if input sequences are padded\n",
        "        # which they are not in this time series use case, because all my\n",
        "        # input sequences are naturally of the same length.\n",
        "        # (https://github.com/huggingface/transformers/issues/4083)\n",
        "        src = self.encoder(src=src)\n",
        "        decoder_output = self.decoder_input_layer(tgt.float())\n",
        "        tgt_mask = None # we are trying to keep it as simple as possible.\n",
        "        decoder_output = self.decoder(tgt=decoder_output.float(), memory=src, tgt_mask=tgt_mask, memory_mask=src_mask)\n",
        "        # print (decoder_output.shape)\n",
        "\n",
        "        decoder_output = self.linear_mapping(decoder_output)\n",
        "        # print(decoder_output.shape)\n",
        "\n",
        "        return decoder_output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hKqatagF7h_4",
        "outputId": "583b8227-3b92-494b-e77a-8a8f33d22aac"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing time_series_transformer.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from importlib import reload\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# load data\n",
        "import pandas as pd\n",
        "from dataset import MyDataset\n",
        "df=pd.read_pickle(drive_prefix+'overall_dataframe2.pkl')\n",
        "df=df.drop(index=df[df.day_id<700].index) # only work on data from 2022 (ish)\n",
        "df.sort_values(['day_id', 'segment_id_int', 'time_window'], inplace=True)\n",
        "top_50_segments = df.groupby(['day_id', 'segment_id_int'])['time_window'].count()\n",
        "top_50_segments = top_50_segments.groupby('segment_id_int').sum().sort_values(ascending=False).head(50)\n",
        "top_50_segments.name=\"time_window_count\"\n",
        "df = df.merge(top_50_segments, left_on=\"segment_id_int\", right_on=\"segment_id_int\").drop(columns=[\"time_window_count\"])\n",
        "\n",
        "df.segment_id_int = df.segment_id_int.factorize()[0]\n",
        "df.day_id=df.day_id.factorize()[0]\n",
        "df.dayofweek=df.dayofweek.factorize()[0]\n",
        "df.dayofweek.factorize()\n",
        "df = df.dropna()  # no idea where 1 na crept up from\n",
        "counts = df.groupby(['day_id', 'segment_id_int'])['time_window'].count()\n",
        "\n",
        "buckets = {}\n",
        "i = 0\n",
        "j = 0\n",
        "for c in counts:\n",
        "  for _ in range(c-5+1):\n",
        "    buckets[j]=range(i,i+5)\n",
        "    i+=1\n",
        "    j+=1\n",
        "  i+=(5-1)"
      ],
      "metadata": {
        "id": "jd6t4SEQR07-"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from time_series_transformer import TimeSeriesTransformer\n",
        "os.environ['CUDA_LAUNCH_BLOCKING']='1'\n",
        "\n",
        "dataset = MyDataset(df, buckets)\n",
        "train_df, test_df = torch.utils.data.random_split(dataset, (0.8, 0.2))\n",
        "\n",
        "from config import dim_val, dec_seq_len, max_seq_len, target_seq_len, n_decoder_layers, n_encoder_layers, n_heads\n",
        "tst = TimeSeriesTransformer(\n",
        "    dim_val=dim_val,\n",
        "    # input_size=input_size,\n",
        "    dec_seq_len=dec_seq_len,\n",
        "    max_seq_len=max_seq_len,\n",
        "    out_seq_len=target_seq_len,\n",
        "    n_decoder_layers=n_decoder_layers,\n",
        "    n_encoder_layers=n_encoder_layers,\n",
        "    n_heads=n_heads\n",
        ")\n",
        "state = torch.load(\"/content/drive/MyDrive/checkpoint_opt.pt\")\n",
        "tst.load_state_dict(state[\"model_state_dict\"])\n",
        "tst.eval()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q5XYjUCZ71cj",
        "outputId": "9166a64f-3153-4d2b-cf27-01f7b82f73dc"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TimeSeriesTransformer(\n",
              "  (time_embedding): Embedding(96, 32)\n",
              "  (segment_embedding): Embedding(50, 128)\n",
              "  (day_embedding): Embedding(7, 7)\n",
              "  (encoder_input_layer): Linear(in_features=8, out_features=345, bias=True)\n",
              "  (positional_encoding_layer): PositionalEncoder(\n",
              "    (dropout): Dropout(p=0.2, inplace=False)\n",
              "  )\n",
              "  (encoder): TransformerEncoder(\n",
              "    (layers): ModuleList(\n",
              "      (0): TransformerEncoderLayer(\n",
              "        (self_attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout1): Dropout(p=0.2, inplace=False)\n",
              "        (dropout2): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (1): TransformerEncoderLayer(\n",
              "        (self_attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout1): Dropout(p=0.2, inplace=False)\n",
              "        (dropout2): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (2): TransformerEncoderLayer(\n",
              "        (self_attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout1): Dropout(p=0.2, inplace=False)\n",
              "        (dropout2): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (3): TransformerEncoderLayer(\n",
              "        (self_attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout1): Dropout(p=0.2, inplace=False)\n",
              "        (dropout2): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (decoder_input_layer): Linear(in_features=1, out_features=512, bias=True)\n",
              "  (decoder): TransformerDecoder(\n",
              "    (layers): ModuleList(\n",
              "      (0): TransformerDecoderLayer(\n",
              "        (self_attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (multihead_attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout1): Dropout(p=0.2, inplace=False)\n",
              "        (dropout2): Dropout(p=0.2, inplace=False)\n",
              "        (dropout3): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (1): TransformerDecoderLayer(\n",
              "        (self_attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (multihead_attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout1): Dropout(p=0.2, inplace=False)\n",
              "        (dropout2): Dropout(p=0.2, inplace=False)\n",
              "        (dropout3): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (2): TransformerDecoderLayer(\n",
              "        (self_attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (multihead_attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout1): Dropout(p=0.2, inplace=False)\n",
              "        (dropout2): Dropout(p=0.2, inplace=False)\n",
              "        (dropout3): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (3): TransformerDecoderLayer(\n",
              "        (self_attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (multihead_attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout1): Dropout(p=0.2, inplace=False)\n",
              "        (dropout2): Dropout(p=0.2, inplace=False)\n",
              "        (dropout3): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (linear_mapping): Linear(in_features=512, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from config import *\n",
        "DELAY_FACTOR = 211.37619472807717\n",
        "\n",
        "test_set = DataLoader(dataset=test_df, batch_size=1, shuffle=False, num_workers=0)\n",
        "for test in test_set:\n",
        "  (src, trg, trg_y) = test\n",
        "  # perform inference\n",
        "  delays = tst(src=src, tgt=trg, src_mask=None, tgt_mask=None)[0, -1, 0].item()\n",
        "  print(\"Delay predicted for this segment is \", delays * DELAY_FACTOR, \"seconds\")\n",
        "  break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AW2wLz4dCaws",
        "outputId": "6fb0efcd-c54c-42ad-870f-a8d7da2c46e1"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Delay predicted for this segment is  39.80212773078741 seconds\n"
          ]
        }
      ]
    }
  ]
}